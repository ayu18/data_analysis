{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbb6bbd7-876c-4d32-8c77-57cbf8f5b03d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "orders = spark.read.format(\"csv\").option(\"header\",True).load(\"/Volumes/dev/scholarnest/spark_data/data_orders.csv\")\n",
    "offers = spark.read.format(\"csv\").option(\"header\",True).load(\"/Volumes/dev/scholarnest/spark_data/data_offers.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66b92b50-0f73-46d2-a69a-023270ae857a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = orders.join(offers,orders.order_gk == offers.order_gk,\"inner\").select(orders['*'],offers['offer_id'])\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04d9bee7-7641-448a-8820-26c462c11217",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "df = (\n",
    "    df\n",
    "    .withColumn(\n",
    "        \"is_driver_assigned\",\n",
    "        F.when(F.col(\"is_driver_assigned_key\") == 1, F.lit(\"Yes\")).otherwise(F.lit(\"No\"))\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"order_status\",\n",
    "        F.when(F.col(\"order_status_key\") == 4, F.lit(\"Client Cancelled\"))\n",
    "         .otherwise(F.lit(\"System Reject\"))\n",
    "    )\n",
    "    .drop(\"is_driver_assigned_key\", \"order_status_key\")\n",
    ")\n",
    "df.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d7c2764-f7d0-4232-9f4e-a5fea959b56a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Build up a distribution of orders according to reasons for failure: cancellations before and after driver assignment, and reasons for order rejection. Analyse the resulting plot. Which category has the highest number of orders?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36570c15-7d9c-473c-b3ea-5a3e53d801bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Aggregate like pivot\n",
    "df_q1 = (\n",
    "    df.groupBy(\"is_driver_assigned\", \"order_status\")\n",
    "      .agg(count(\"order_gk\").alias(\"order_count\"))\n",
    ")\n",
    "\n",
    "# Step 2: Convert to Pandas for pivot + plotting\n",
    "pdf_q1 = df_q1.toPandas()\n",
    "\n",
    "# Step 3: Pivot the pandas DataFrame (wide format)\n",
    "pivot_q1 = pdf_q1.pivot_table(\n",
    "    columns=[\"order_status\",\"is_driver_assigned\"],\n",
    "    values=\"order_count\",\n",
    "    aggfunc=\"sum\",\n",
    "    fill_value=0\n",
    ")\n",
    "\n",
    "# Step 4: Plot the bar chart\n",
    "pivot_q1.plot(\n",
    "    kind=\"bar\",\n",
    "    figsize=(7, 7),\n",
    "    legend=True,\n",
    "    rot=0\n",
    ")\n",
    "plt.title(\"Order Counts by Driver Assignment and Order Status\")\n",
    "plt.ylabel(\"Count of Orders\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f0633f8-eab6-401c-960d-0259e79cf6be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Plot the distribution of failed orders by hours. Is there a trend that certain hours have an abnormally high proportion of one category or another? What hours are the biggest fails? How can this be explained? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb775b49-1734-4925-9c15-c2269e8ce4bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# df_time = df.withColumn('order_time',F.col('order_datetime').split(\":\")[0])\n",
    "df_time = df.withColumn(\n",
    "    \"order_time\",\n",
    "    F.split(F.col(\"order_datetime\"), \":\")[0]   # takes part before first \":\"\n",
    ")\n",
    "df_time.display()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f66819c-940a-4b64-8237-ec53aca8019f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_time_grouped = df_time.groupby(\"order_time\",\"is_driver_assigned\",\"order_status\").agg(count(\"order_gk\").alias(\"order_gk\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce191da5-74b3-48e6-b99c-f55383c6b743",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "grouped_q2 = df_time_grouped.toPandas()\n",
    "_ = grouped_q2.reset_index().pivot(index=\"order_time\",\n",
    "                                   columns=[\"is_driver_assigned\", \"order_status\"],\n",
    "                                   values=\"order_gk\").plot(xticks=range(0, 24),\n",
    "                                                           figsize=(13, 7),\n",
    "                                                           title=\"Count of Failed Orders Per Hour and Category\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "096c2467-1bb2-40c3-bfbe-48da4e7774d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Plot the average time to cancellation with and without driver, by hour. Can we draw any conclusions from this plot?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21c4b8fd-2a5f-4180-ae0d-92b39e0470df",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1759396860085}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import mean\n",
    "df_cancellation_grouped = df_time.groupby(\"order_time\",\"is_driver_assigned\").agg(mean(\"cancellations_time_in_seconds\").alias(\"cancellations_time_in_seconds\"))\n",
    "df_cancellation_grouped.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b156eee-8c87-487d-a4d3-5c0c93fc66e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "grouped_q3 = df_cancellation_grouped.toPandas()  \n",
    "_ = grouped_q3.reset_index().pivot(index=\"order_time\",\n",
    "                                   columns=\"is_driver_assigned\",\n",
    "                                   values=\"cancellations_time_in_seconds\").plot(xticks=range(0, 24),\n",
    "                                                                                figsize=(13, 7),\n",
    "                                                                                title=\"Average Time to Cancellation Per Hour and Driver Assignment\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Assign-1",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
